## Материалы модуля 10

<div align="center">
  <img src="../images/dls.png">
</div>

### Speech LLMs: Audio-Conditioned Models
Традиционные речевые пайплайны (Cascade) постепенно уступают место мультимодальным подходам. В этом блоке мы переходим к Cutting Edge теме — Speech LLMs. Мы обсудим, как наделить большие языковые модели слухом, превратив их в Audio-Conditioned LLMs. Разберем, чем отличается семантическое представление звука от акустического, как "подружить" аудио-энкодер с текстовым декодером и почему End-to-End подходы открывают новые возможности (Emergent Abilities), недоступные классическим связкам ASR + NLP.


### Лекция
В лекции мы погрузимся в архитектуру языковых моделей с нативным пониманием аудио - Audio-Conditioned LLMs (Audio-In -> Text-Out). Лекция структурно разделена на теоретические основы и архитектурные решения.
В первой части обсудим эволюцию подходов: от каскадных систем (ASR + LLM + TTS) к единым End-to-End решениям. Разберем, зачем вообще нужны Audio-LLM и какие "эмерджентные" способности у них появляются. Во второй части детально разберем "анатомию" мультимодальной модели:
Представления (Representations): В чем разница между семантическими (Whisper, HuBERT) и акустическими (EnCodec, CLAP) признаками и для каких задач они лучше подходят.
Архитектура: Как подать аудио в LLM? Сравним подходы с расширением словаря (Audio Tokens), Cross-Attention (на примере Audio-Flamingo) и самый популярный метод — добавление эмбеддингов в промпт (Soft Prompts).
Компоненты: Разберем выбор энкодеров, роль проекторов (Projectors/Adapters) и методы сжатия (Subsampling) последовательности.
SOTA модели: Обзор архитектурных решений в Qwen-Audio, SALMONN и разбор кейса обучения Voxtral

### Семинар

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepLearningSchool/Speech/blob/main/week_11_speech_low_resource_languages/Practice/dls-audiollm-seminar.ipynb)

На семинаре мы перейдем от теории к практике работы с Audio-Conditioned LLMs. 
Мы рассмотрим пайплайн инференса модели Ultravox, ее архитектуру и тонкости формирования промпта.
Затем мы реализуем AudioChatLLaMA-like подход к генерации синтетических данных для обучения AudioLLMs, нагенерируем небольшой датасет и протестируем технику small-batch overfitting, которая позволяет проверить корректность пайплайна обучения и отловить баги реализации без запуска долгих и тяжелых обучений.


### Домашнее задание

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepLearningSchool/Speech/blob/main/week_11_speech_low_resource_languages/Homework/audiollm-hw.ipynb)

В домашнем задании будет предложено проделать похожие с семинаром действия, но с углублением в архитектуру модели:
1. Реализовать подход генерации синтетических AudioQA-данных на основе ASR-датасета
2. Имплементировать некоторые части в классе архитектуры модели и датасета
3. Сделать small-batch overfit для модели с архитектурой, которую вы реализовали
4. Подгрузить заранее предобученную авторами модель в той же архитектуре и написать для нее функции генерации и MMLU-like валидации