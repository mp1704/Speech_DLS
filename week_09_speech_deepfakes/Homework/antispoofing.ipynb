{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 3842332,
          "sourceType": "datasetVersion",
          "datasetId": 2286778
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Antispoofing\n",
        "\n",
        "In this homework, you will develop a countermeasure against deepfakes and then try to explain it using various XAI techniques.\n",
        "\n",
        "More specifically, you will implement and train a Countermeasure (CM) system on the Logical Access partition of the [ASVSpoof 2019 Dataset](https://datashare.ed.ac.uk/handle/10283/3336) ([Kaggle Link](https://www.kaggle.com/datasets/awsaf49/asvpoof-2019-dataset)). You may find the [ASVspoof 2019 evaluation plan](https://www.asvspoof.org/asvspoof2019/asvspoof2019_evaluation_plan.pdf) useful.\n",
        "\n",
        "For the CM, we choose [LightCNN (LCCN)](https://arxiv.org/abs/1511.02683) that once achieved the top place in the competition. We will follow the Speech Technology Center (STC) [paper](https://arxiv.org/abs/1904.05576).\n",
        "\n",
        "**Hints**:\n",
        "\n",
        "1. Use STFT (FFT in the paper) as front-end.\n",
        "\n",
        "2. The dropout layer is put before the last batch norm."
      ],
      "metadata": {
        "id": "LQqClR4wD3i-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset [0.5 pts]\n",
        "\n",
        "We want to train a neural network to predict if the input audio is real or fake. To do so, we need a dataset first. In this homework, we will work with [ASVspoof19](https://arxiv.org/pdf/1911.01601.pdf).\n",
        "\n",
        "Create a `Dataset` class that downloads the dataset, parses its metadata and, given index $i$, returns $i$-th object of the dataset. Do not forget to preprocess audio for LCNN (calculate stft, etc.)."
      ],
      "metadata": {
        "id": "aMYla0SwEOlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "T7qxaDmUqbKf",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hint**: when working in Kaggle, it is easier and faster to use dataset as kaggle input. We can use it directly or add a symlink to a local dir using `ln -s`.\n",
        "**Hint**: it might be easier to do this homework in Kaggle, since model training may take some time"
      ],
      "metadata": {
        "id": "dPTqyxuG4KME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create train/eval dataset and dataloaders:"
      ],
      "metadata": {
        "id": "P3Ne_-ukTI9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "KWcEeyTXUNxJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-22T07:30:11.421423Z",
          "iopub.execute_input": "2025-10-22T07:30:11.421685Z",
          "iopub.status.idle": "2025-10-22T07:30:11.426354Z",
          "shell.execute_reply.started": "2025-10-22T07:30:11.421666Z",
          "shell.execute_reply": "2025-10-22T07:30:11.425652Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize one object, just to check that all is fine:"
      ],
      "metadata": {
        "id": "3TWcP9muTTY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "dOME0SO3UB1s",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-22T07:30:12.123610Z",
          "iopub.execute_input": "2025-10-22T07:30:12.123873Z",
          "iopub.status.idle": "2025-10-22T07:30:12.131732Z",
          "shell.execute_reply.started": "2025-10-22T07:30:12.123853Z",
          "shell.execute_reply": "2025-10-22T07:30:12.131059Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function [0.5 pts]"
      ],
      "metadata": {
        "id": "LsQsMSrFXF10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the lecture, we saw different softmax losses and the motivation behind them for the ASV task. However, they can also be used for any classification task, such as synthesized speech detection. The papers suggest to use A(M)-Softmax or Cross-Entropy. The STC paper argues that A-Softmax is better.\n",
        "\n",
        "(a) Explain what are the benefits of A-softmax over cross-entropy according to the STC paper?\n",
        "\n",
        "(b) Analyse the [NII paper](https://arxiv.org/pdf/2103.11326) and explain if complicated Softmax is actually needed to achieve good EER or we can go with Cross Entropy."
      ],
      "metadata": {
        "id": "u-J7Dh03Xi0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: your answer here..."
      ],
      "metadata": {
        "id": "stMR8RDqYYnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following tha NII paper, we will continue with Cross Entropy"
      ],
      "metadata": {
        "id": "Cvnr8PFbbfYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "6j1m-0olbeuQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-22T07:30:17.770031Z",
          "iopub.execute_input": "2025-10-22T07:30:17.770577Z",
          "iopub.status.idle": "2025-10-22T07:30:17.774030Z",
          "shell.execute_reply.started": "2025-10-22T07:30:17.770554Z",
          "shell.execute_reply": "2025-10-22T07:30:17.773294Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation metric [0.5 pts]\n",
        "\n",
        "We will use equal error rate as the primary evaluation metric. The code for calculating metrics is provided by the ASVspoof itself. We just need to write a wrapper. Given model logits and labels, calculate EER using the ASVspoof functions.\n",
        "\n",
        "Your model returns two probas: [spoof_proba, bona_proba]. Be careful with the EER metric and recal how ROC curve is computed to ensure that you do not make a mistake."
      ],
      "metadata": {
        "id": "gK_4HPWLqle6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download asvspoof metric calculation functions\n",
        "!wget https://raw.githubusercontent.com/markovka17/dla/refs/heads/2023/hw5_as/calculate_eer.py"
      ],
      "metadata": {
        "id": "bSeOnQRCrBwm",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-22T07:30:18.289231Z",
          "iopub.execute_input": "2025-10-22T07:30:18.289699Z",
          "iopub.status.idle": "2025-10-22T07:30:18.607921Z",
          "shell.execute_reply.started": "2025-10-22T07:30:18.289679Z",
          "shell.execute_reply": "2025-10-22T07:30:18.607201Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from calculate_eer import compute_eer\n",
        "\n",
        "def get_eer(logits, labels):\n",
        "    # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "8I62HHtTqkyZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-22T07:30:18.609524Z",
          "iopub.execute_input": "2025-10-22T07:30:18.609809Z",
          "iopub.status.idle": "2025-10-22T07:30:18.617358Z",
          "shell.execute_reply.started": "2025-10-22T07:30:18.609788Z",
          "shell.execute_reply": "2025-10-22T07:30:18.616764Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LCNN Implementation [6.0 pts]\n",
        "\n",
        "Create a `LCNN` class for the model architecture."
      ],
      "metadata": {
        "id": "Fjp9VVQKW8Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "T0qMJXJtXOBH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-22T07:30:19.772531Z",
          "iopub.execute_input": "2025-10-22T07:30:19.773226Z",
          "iopub.status.idle": "2025-10-22T07:30:19.783069Z",
          "shell.execute_reply.started": "2025-10-22T07:30:19.773201Z",
          "shell.execute_reply": "2025-10-22T07:30:19.782313Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = LCNN(...)"
      ],
      "metadata": {
        "id": "Nx-CfmTMtAlf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-22T07:30:22.499277Z",
          "iopub.execute_input": "2025-10-22T07:30:22.499907Z",
          "iopub.status.idle": "2025-10-22T07:30:22.596820Z",
          "shell.execute_reply.started": "2025-10-22T07:30:22.499867Z",
          "shell.execute_reply": "2025-10-22T07:30:22.596243Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# double-check that it runs (do eval mode)\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "SQR8G7lftINv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-22T07:30:22.721847Z",
          "iopub.execute_input": "2025-10-22T07:30:22.722112Z",
          "iopub.status.idle": "2025-10-22T07:30:23.471725Z",
          "shell.execute_reply.started": "2025-10-22T07:30:22.722096Z",
          "shell.execute_reply": "2025-10-22T07:30:23.470934Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the train loop. Since it may take some time, we advise you to save your checkpoints after each epoch to load it back if needed.\n",
        "\n",
        "Plot EER vs epoch and loss vs epoch curves"
      ],
      "metadata": {
        "id": "4fsAW6IIXOml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, scheduler, device):\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, eval_dataloader, criterion, optimizer, scheduler, device, n_epochs):\n",
        "    # YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "ADvC5FIbUL-N",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-22T07:30:23.473128Z",
          "iopub.execute_input": "2025-10-22T07:30:23.473590Z",
          "iopub.status.idle": "2025-10-22T07:30:23.484471Z",
          "shell.execute_reply.started": "2025-10-22T07:30:23.473572Z",
          "shell.execute_reply": "2025-10-22T07:30:23.483853Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "yVIAsdprDNit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take optimizer and scheduler from the NII paper\n",
        "optimizer = # YOUR CODE HERE\n",
        "scheduler = # YOUR CODE HERE\n",
        "n_epochs = # YOUR CODE HERE\n",
        "train(model, train_dataloader, eval_dataloader,\n",
        "      criterion, optimizer, scheduler, device, n_epochs)"
      ],
      "metadata": {
        "id": "ch6C-P-3wY15",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-22T07:30:23.485165Z",
          "iopub.execute_input": "2025-10-22T07:30:23.485407Z",
          "iopub.status.idle": "2025-10-22T10:35:21.834398Z",
          "shell.execute_reply.started": "2025-10-22T07:30:23.485386Z",
          "shell.execute_reply": "2025-10-22T10:35:21.833681Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task is consired solved if you achieve at least $9\\%$ EER. It is much higher than the model can achieve but we do not want you to wait 12+ hours for the model to converge."
      ],
      "metadata": {
        "id": "rurTwYbLvK9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XAI [See points below]\n",
        "\n",
        "Let's analyse the model we have created. We won't be able to understand the differences easily without having some reference. So, we will use the novel idea from the recent [Interspeech 2025 paper](https://arxiv.org/abs/2506.03425).\n",
        "\n",
        "We will use a [vocoded dataset](https://arxiv.org/abs/2210.10570) of parallel samples: real and fake audio have the same speaker saying the same content at the same time. The ground-truth explanation will be obtained by calculating difference between real and fake spectrograms."
      ],
      "metadata": {
        "id": "qdxERC6nvbPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/records/7314976/files/project09-voc.v4.tar?download=1 -O project09-voc.v4.tar\n",
        "!tar -xvf project09-voc.v4.tar"
      ],
      "metadata": {
        "id": "QbdVgqAe1KFw",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that real audio is taken from ASVspoof. So let's take a real example from the asvspoof dataset. Using its filename, find the corresponding `hifi-gan` and `waveglow` vocoded versions in the vocv4 and load them too\n",
        "\n",
        "In reality, we are interested in the explanations for the unseen data. But for this homework, let's consider the train set. This will allow us to see if the model learns the futures we expect it to learn (assuming the XAI tool is trustworthy) (Though spoof part of vocv4 is not exactly the same as the one in asvspoof, so we mostly eliminate the issues related to changing speakers, not algorithms)"
      ],
      "metadata": {
        "id": "yyEYBTWO4164"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ind = # for consistency with solutions choose the index that corresponds to LA_T_4179989 (bona fide)\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "--sJdaDt4y5x",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# get LCNN-prepared spectrogram for the paired real and fake example from the dataset\n",
        "# paired: the same filename, but one is bona fide another is created via vocoder\n",
        "real_audio = # YOUR CODE HERE\n",
        "hifigan_audio = # YOUR CODE HERE\n",
        "waveglow_audio = # YOUR CODE HERE\n",
        "\n",
        "\n",
        "# fake audio may be slightly longer due to padding, remove some part from the end to make the length equal\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# preprocess audio for model input\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "mnFJIGRy1OXJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run your model on these clips. See if the model prediction is correct. Use this understanding for the following analysis"
      ],
      "metadata": {
        "id": "lCJWENgdGSLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "WLTC-I2DDmE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual explanation [0.5 pts]\n",
        "\n",
        "Compare the two spectrograms (real and fake). What differences do you see? (**Hint**: they exist, if you do not see -- look carefully)."
      ],
      "metadata": {
        "id": "txMSS3WL3F3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "HShXw6PW3Ppi",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer here**"
      ],
      "metadata": {
        "id": "qwO3TDEB3Z0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic explanation. [0.5 pts]\n",
        "\n",
        "Calculate Eq. 2 from the [Interspeech 2025 paper](https://arxiv.org/abs/2506.03425) to automatically highlight the differences between two objects"
      ],
      "metadata": {
        "id": "bAqmCFOt3ejI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Exb191_j303f",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the mask on top of the fake spectrogram and compare three plots: real, fake, fake+mask on top. Do it for both vocoders. Compare"
      ],
      "metadata": {
        "id": "KyM7wCB-6Jkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "B3d2ZE_U6REK",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your comparison of the ground-truth mask with your manual analysis (from previous subtask) here**"
      ],
      "metadata": {
        "id": "AV_JX2KE_HaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grad-CAM [0.5 pts]\n",
        "\n",
        "Using [pytorch-grad-cam lib](https://github.com/jacobgil/pytorch-grad-cam), implement [Grad-CAM](https://arxiv.org/abs/1610.02391) for your LCNN model. Choose the layer you like"
      ],
      "metadata": {
        "id": "ZoBVNkR3vr3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grad-cam and captum may have conflicting numpy dependencies. Just install grad-cam first, then captum and it will work"
      ],
      "metadata": {
        "id": "_aIGs9DSuj-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam"
      ],
      "metadata": {
        "id": "QMqapTz3AYfx",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "VaXEIRE1_3JD",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison [1.0 pts]\n",
        "\n",
        "Compare your Grad-CAM attributions with another gradient-based method: InputXGradient. Compute it using [Captum](https://captum.ai/)."
      ],
      "metadata": {
        "id": "ow3HtHlpv_6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install captum"
      ],
      "metadata": {
        "id": "k0NHpx3jwOwg",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "OuI4EXwewQr6",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sV8osNC5INWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do three plots: mask vs grad-cam vs inputXgradient. Compare them. Does any of the XAI methods align with the mask?"
      ],
      "metadata": {
        "id": "Bt5dpJx4_oeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "XuvtX9J2wWUY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your answer here**"
      ],
      "metadata": {
        "id": "iC_VXcn8JzQy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "abcldpncxVs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to skewed distribution for some XAI tools, you may want to look at top-5% points, similarly to the ground-truth mask. Binarize attributions using their $95\\%$ quantile and plot again:"
      ],
      "metadata": {
        "id": "ErN-xDi5vC5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "XhNLtl8cIfLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your analysis here**"
      ],
      "metadata": {
        "id": "3PWtNVvjvrLS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3Hx_uH1viOt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}